\section{Discussion}

\subsection{Model Performance Analysis}
The results demonstrate a clear performance hierarchy, with the neural network architecture achieving the best results:
\begin{enumerate}
    \item \textbf{SE-ResNet neural network} achieved the best performance (8.6047 RMSE), outperforming all gradient boosting methods by 1.74\%.
    \item \textbf{Gradient boosting ensembles} (stacking, weighted average) achieved $\sim$8.76 RMSE, with diminishing returns from ensemble complexity.
    \item \textbf{CatBoost} was the best single tree-based model (8.7618), slightly outperforming LightGBM due to its sophisticated categorical handling.
    \item \textbf{XGBoost} underperformed with higher variance, potentially due to suboptimal hyperparameters.
\end{enumerate}

\subsection{Why Neural Networks Outperform GBMs}
The SE-ResNet's 1.74\% improvement over gradient boosting is notable and stems from several factors:

\textbf{1. Data Augmentation:}
The most significant advantage comes from incorporating the original dataset (not just the Kaggle competition data) during training. This effectively increases the training set size and provides additional signal. GBMs were trained only on competition data, while the neural network benefited from augmented data.

\textbf{2. Learned Feature Interactions:}
While GBMs capture feature interactions through tree splits, neural networks learn continuous, differentiable interaction functions. The SE-ResNet's residual blocks with attention mechanisms can model complex, non-linear relationships that axis-aligned tree splits may miss.

\textbf{3. Entity Embeddings:}
Categorical embeddings (dimension $d=8$) allow the network to learn distributed representations where similar categories are mapped to nearby vectors. This provides richer representations than one-hot encoding or target encoding used in GBMs.

\textbf{4. Attention Mechanisms:}
Squeeze-and-Excitation blocks dynamically reweight features for each sample, effectively performing instance-specific feature selection. This adaptive behavior is not possible with static feature importance in GBMs.

\textbf{5. Regularization Strategy:}
The combination of dropout (11\%), weight decay, LayerNorm, and early stopping provides robust regularization that prevents overfitting despite the model's high capacity.

\subsection{Feature Engineering Impact}
The engineered \texttt{formula\_score} feature, based on domain knowledge from competition discussions, became the single most important predictor.
This highlights the value of incorporating domain expertise and exploring competition forums for insights.

Target encoding and interaction features (especially \texttt{study\_attendance}) contributed significantly, demonstrating that careful feature engineering can outweigh model selection in tabular competitions.

\subsection{Error Analysis}
Residual analysis revealed:
\begin{itemize}
    \item Near-zero mean residual (0.0415) indicates minimal systematic bias.
    \item Residual standard deviation (8.7567) aligns with the cross-validation RMSE, indicating consistent error magnitude.
    \item Residual range of [-43.35, +48.15] suggests difficulty predicting extreme scores.
    \item Largest errors likely occur for students with unusual combinations of features.
\end{itemize}

\subsection{Gradient Boosting vs.\ Neural Networks: Trade-offs}
While SE-ResNet achieved the best results, gradient boosting methods remain valuable:
\begin{itemize}
    \item \textbf{Training efficiency}: GBMs train in minutes; the neural network required hours.
    \item \textbf{Interpretability}: SHAP values provide clear explanations for GBMs; neural network explanations are less straightforward.
    \item \textbf{Hyperparameter sensitivity}: Neural networks require careful tuning of learning rate, architecture depth, dropout, etc.
    \item \textbf{Data requirements}: Neural networks benefit more from data augmentation; GBMs are more data-efficient.
\end{itemize}

\subsection{Limitations}
Several limitations apply to this work:
\begin{itemize}
    \item \textbf{Synthetic data}: The dataset was generated from a deep learning model, so relationships may not perfectly reflect real-world patterns. Notably, the neural network's advantage may partly stem from matching the data-generating process.
    \item \textbf{Data augmentation fairness}: The neural network used additional data (original dataset), which was not available to GBM models. A fairer comparison would train all models with the same augmented data.
    \item \textbf{Limited neural network exploration}: Only one architecture was tested; alternatives like TabNet, FT-Transformer, or NODE could yield further improvements.
    \item \textbf{Generalization}: Results are specific to this synthetic dataset and may not transfer to real student data.
\end{itemize}

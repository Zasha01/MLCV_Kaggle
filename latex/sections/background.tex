\section{Technical Background}

This work addresses a supervised learning problem in the form of regression. Given a dataset
$\{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$, where $\mathbf{x}_i \in \mathbb{R}^d$ denotes a feature vector
describing a student and $y_i \in \mathbb{R}$ represents the corresponding exam score, the goal is
to learn a function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ that accurately predicts exam scores
for unseen samples.

\subsection{Supervised Regression}
Regression problems aim to predict continuous-valued targets and are well suited for modeling
exam scores, where both ordering and magnitude of prediction errors are meaningful. Unlike
classification, regression allows the model to capture fine-grained differences between student
outcomes, which is essential for performance evaluation in this task.

\subsection{Evaluation Metric}
Model performance is evaluated using Root Mean Squared Error (RMSE), defined as
\begin{equation}
\mathrm{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2},
\end{equation}
where $y_i$ is the true exam score and $\hat{y}_i$ is the predicted value. RMSE penalizes larger
errors more heavily than absolute-error metrics, making it particularly suitable for detecting
poor predictions on extreme exam scores. The metric is commonly used in regression benchmarks
and is the official evaluation criterion of the Kaggle competition.

\subsection{Linear Models and Regularization}
Linear regression models assume a linear relationship between the input features and the target:
\begin{equation}
\hat{y} = \mathbf{w}^\top \mathbf{x} + b.
\end{equation}
While simple and interpretable, linear models often struggle with nonlinear interactions present
in real-world tabular data. Ridge Regression extends linear regression by adding an $L_2$
regularization term to the loss function, discouraging large coefficients and reducing overfitting,
particularly in high-dimensional feature spaces.

\subsection{Tree-Based Models}
Decision trees model nonlinear relationships by recursively partitioning the feature space based
on threshold splits. They naturally handle mixed numerical and categorical data but are prone to
high variance when used individually. Ensemble methods mitigate this issue by combining multiple
trees to improve generalization.

\subsection{Gradient Boosted Decision Trees}
Gradient boosting is a powerful ensemble technique that builds models sequentially, where each
new tree is trained to correct the residual errors of the previous ensemble. This process can be
interpreted as gradient descent in function space, minimizing a specified loss function. Gradient
Boosted Decision Trees (GBDTs) are particularly effective for tabular datasets due to their ability
to capture nonlinear feature interactions and handle heterogeneous data types.

Popular GBDT implementations include XGBoost, LightGBM, and CatBoost. While all follow the same
boosting principle, they differ in optimization strategies and handling of categorical features.
CatBoost, in particular, incorporates ordered boosting and native categorical feature processing,
often resulting in improved performance on datasets with many categorical variables.

\subsection{Ensemble Learning}
Ensemble learning combines predictions from multiple models to achieve improved robustness and
generalization. By aggregating models with different inductive biases, ensembles reduce variance
and exploit complementary strengths. In this work, both simple averaging, weighted ensembling,
and stacking with a meta-learner are considered to further enhance predictive performance.

\subsection{Deep Learning for Tabular Data}
While gradient boosting methods have traditionally dominated tabular data competitions, recent
advances in neural network architectures have shown competitive or superior performance.
Key innovations enabling this include:

\textbf{Entity Embeddings:} Categorical features are mapped to dense vector representations
through learnable embedding layers, allowing the network to discover semantic relationships
between categories. For a categorical feature with $K$ unique values, an embedding layer
learns a matrix $\mathbf{E} \in \mathbb{R}^{K \times d}$ where $d$ is the embedding dimension.

\textbf{Residual Connections:} Deep networks benefit from skip connections that allow gradients
to flow directly through the network, enabling training of deeper architectures without
degradation. A residual block computes:
\begin{equation}
\mathbf{h}_{l+1} = \mathbf{h}_l + \mathcal{F}(\mathbf{h}_l; \theta_l),
\end{equation}
where $\mathcal{F}$ represents learnable transformations.

\textbf{Squeeze-and-Excitation (SE) Blocks:} Originally proposed for image classification,
SE blocks adaptively recalibrate feature responses by modeling channel interdependencies.
Given a feature vector $\mathbf{x} \in \mathbb{R}^C$, the SE mechanism computes attention
weights via a bottleneck:
\begin{equation}
\mathbf{s} = \sigma(\mathbf{W}_2 \cdot \text{ReLU}(\mathbf{W}_1 \cdot \mathbf{x})),
\end{equation}
where $\mathbf{W}_1 \in \mathbb{R}^{C/r \times C}$ and $\mathbf{W}_2 \in \mathbb{R}^{C \times C/r}$
with reduction ratio $r$. The output is $\mathbf{x} \odot \mathbf{s}$, allowing the network to
emphasize informative features and suppress less useful ones.

\subsection{Model Interpretability with SHAP}
While gradient boosting models achieve strong predictive performance on tabular data, their
decision-making process is often opaque. To address this, we employ SHAP (SHapley Additive
exPlanations)~\cite{lundbergUnifiedApproachInterpreting2017}, a unified framework for interpreting
model predictions based on game-theoretic Shapley values~\cite{shapleyValueNPersonGames1953}.

SHAP values quantify the contribution of each feature to an individual prediction. For a prediction
$\hat{y}$, the SHAP value $\phi_j$ for feature $j$ satisfies the efficiency property:
\begin{equation}
\hat{y} = \phi_0 + \sum_{j=1}^{d} \phi_j,
\end{equation}
where $\phi_0$ is the expected prediction across the training data. This decomposition ensures that
feature contributions sum exactly to the model output, providing faithful explanations.

Tree-based models enable efficient SHAP computation via TreeExplainer~\cite{lundbergLocalExplanationsGlobal2020},
which exploits the tree structure to calculate exact Shapley values in polynomial time. This allows
us to generate both local explanations (why did the model predict this score for a specific student?)
and global insights (which features matter most across all predictions?).

Global feature importance is derived by aggregating local SHAP values:
\begin{equation}
I_j = \frac{1}{N} \sum_{i=1}^{N} |\phi_j^{(i)}|,
\end{equation}
where $\phi_j^{(i)}$ is the SHAP value of feature $j$ for sample $i$. Unlike permutation importance,
SHAP-based importance accounts for feature interactions and provides directional information about
how features influence predictions~\cite{lundbergExplainableMachinelearningPredictions2018}.

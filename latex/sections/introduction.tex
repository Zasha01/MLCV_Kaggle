\section{Introduction}
Predicting academic performance from student- and study-related factors is a practical regression problem with applications in personalized learning and early identification of students who may benefit from intervention.
From a modeling perspective, the relationship between behavioral variables such as study time, attendance, and sleep and exam outcomes is often nonlinear and shaped by interactions, which makes the task a useful benchmark for tabular machine learning methods.

We evaluate our methods on the Kaggle Playground Series S6E1 benchmark for predicting student exam scores using study habits and contextual factors.

The dataset is synthetically generated from a model trained on real student performance data, providing a consistent benchmark for evaluating tabular learning methods.

Our experiments compare three model families: classical baselines (e.g., linear and random-forest regressors), gradient-boosted decision trees, and neural architectures for tabular data.
We study how feature engineering and hyperparameter optimization affect these families under cross-validation, and we complement performance evaluation with interpretability analyses based on feature importance, SHAP, and prediction error patterns.

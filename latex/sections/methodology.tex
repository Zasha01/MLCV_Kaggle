\section{Methodology}
Our modeling pipeline progresses from tabular baselines to tuned gradient boosting models, ensembling, and a neural architecture for tabular data.


\subsection{Validation Strategy}
All experiments use 5-fold cross-validation with shuffling (random seed 42) to obtain reliable performance estimates and reduce overfitting.
We reuse the same fold splits across all models to ensure fair comparison, and we store out-of-fold (OOF) predictions for ensemble construction and weight optimization.


\subsection{Feature Engineering}
Starting from 12 original features, we construct a set of 44 features to capture nonlinearities, interactions, and category-specific effects.
For numerical variables, we include pairwise interaction terms (e.g., \texttt{study\_hours} $\times$ \texttt{class\_attendance}), ratio features (e.g., attendance-per-hour), and polynomial expansions (squared and cubic terms) for the most relevant predictors.
We additionally introduce threshold-based indicator variables (e.g., low sleep and high study intensity) and a composite score based on domain intuition:
\begin{equation}
S = 6.0h + 0.35a + 1.5s,
\label{eq:domain_score}
\end{equation} where $h=\texttt{study\_hours}$, $a=\texttt{class\_attendance}$, and $s=\texttt{sleep\_hours}$.
Categorical variables are encoded using label encoding and augmented with CV-safe target encoding and frequency encoding to expose category-specific signal while avoiding leakage.


\subsection{Models Evaluated}
We evaluate four groups of regressors: classical baselines, gradient-boosted decision trees, ensemble methods, and a neural model for tabular data.

\subsubsection{Baselines}
As classical baselines, we train Ridge regression ($\alpha=10.0$) on standardized features and a Random Forest regressor with 100 trees (max depth 15, minimum leaf size 10).

\subsubsection{Gradient-Boosted Decision Trees}
We benchmark three gradient-boosted decision tree implementations: LightGBM, XGBoost, and CatBoost.
Early stopping is applied where supported to mitigate overfitting, and final model settings are selected either from baseline configurations or via the tuning procedure described in Section~\ref{sec:hpo}.

\subsubsection{Ensemble Methods}
We evaluate three ensemble strategies built from the gradient-boosted models.
These include a simple mean blend, a weighted blend with weights optimized on out-of-fold predictions using Nelder--Mead, and stacking with a Ridge regression meta-learner.

\subsubsection{Neural Tabular Model}
Finally, we evaluate SE-ResNet, a neural architecture for mixed categorical--numerical data that combines entity embeddings with residual blocks and Squeeze-and-Excitation attention.


\subsection{SE-ResNet Architecture}
To model nonlinear interactions in mixed categorical--numerical inputs, we employ a tabular neural network based on residual learning with Squeeze-and-Excitation (SE) attention.
Numerical features are standardized and augmented with selected engineered transformations, while each categorical feature is mapped to a learnable embedding.
All inputs are concatenated and projected to a fixed hidden width, followed by multiple residual blocks with channel-wise attention.
The model outputs a single scalar prediction for \texttt{exam\_score}.
In addition to the composite score $S$ defined in Eq.~\eqref{eq:domain_score}, we include a calibrated variant as an additional neural input:
\begin{equation}
\begin{aligned}
\tilde{S} &= 5.91h + 0.35a + 1.42s + 4.78,
\end{aligned}
\label{eq:domain_score_nn}
\end{equation}
where $h=\texttt{study\_hours}$, $a=\texttt{class\_attendance}$, and $s=\texttt{sleep\_hours}$. Table~\ref{tab:seresnet_config} summarizes the SE-ResNet architecture and training configuration used in our experiments.

\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.1}
\caption{SE-ResNet configuration used in our experiments.}
\label{tab:seresnet_config}
\begin{tabular}{l l}
\hline
\textbf{Component} & \textbf{Setting} \\
\hline
Categorical representation & entity embeddings, $d=8$ per feature \\
Input projection & 256 hidden dimensions \\
Residual blocks & 3 blocks \\
Block structure & LayerNorm $\rightarrow$ Linear $\rightarrow$ ReLU $\rightarrow$ Dropout \\
Dropout & 0.11 \\
SE module & reduction ratio $r=4$ \\
Prediction head & 256 $\rightarrow$ 64 $\rightarrow$ 16 $\rightarrow$ 1 \\
Optimizer & AdamW (lr=$10^{-3}$, weight decay=$10^{-4}$) \\
LR scheduler & ReduceLROnPlateau (factor 0.5, patience 10) \\
Early stopping & patience 20 epochs \\
Batch size & 256 (train), 1024 (validation) \\
\hline
\end{tabular}
\end{table}




\subsection{Hyperparameter Tuning}
\label{sec:hpo}
We tuned the gradient boosting models using Bayesian optimization with Optuna~\cite{optuna}.
For each model, we ran 50 trials and evaluated each trial using 5-fold cross-validation, minimizing the RMSE.
The hyperparameter ranges explored for LightGBM, XGBoost, and CatBoost are summarized in Table~\ref{tab:search_space}.
Unless stated otherwise, parameters were sampled uniformly within their ranges, while \texttt{learning\_rate}, \texttt{reg\_alpha}, and \texttt{reg\_lambda} were sampled log-uniformly to better cover multiple orders of magnitude.
The best-performing configurations selected by Optuna for each model are reported in Table~\ref{tab:tuned_params}.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.1}
\caption{Optuna search spaces used for Bayesian hyperparameter optimization.}
\label{tab:search_space}
\begin{tabular}{l l l}
\hline
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Range} \\
\hline
\multirow{7}{*}{LightGBM}
  & \texttt{num\_leaves}         & $[20, 100]$ \\
  & \texttt{learning\_rate}      & $[0.01, 0.1]$ \\
  & \texttt{feature\_fraction}   & $[0.6, 1.0]$ \\
  & \texttt{bagging\_fraction}   & $[0.6, 1.0]$ \\
  & \texttt{min\_child\_samples} & $[5, 50]$ \\
  & \texttt{reg\_alpha}          & $[10^{-3}, 10]$ \\
  & \texttt{reg\_lambda}         & $[10^{-3}, 10]$ \\
\hline
\multirow{6}{*}{XGBoost}
  & \texttt{max\_depth}          & $[3, 10]$ \\
  & \texttt{learning\_rate}      & $[0.01, 0.1]$ \\
  & \texttt{subsample}           & $[0.6, 1.0]$ \\
  & \texttt{colsample\_bytree}   & $[0.6, 1.0]$ \\
  & \texttt{reg\_alpha}          & $[10^{-3}, 10]$ \\
  & \texttt{reg\_lambda}         & $[10^{-3}, 10]$ \\
\hline
\multirow{4}{*}{CatBoost}
  & \texttt{depth}               & $[4, 10]$ \\
  & \texttt{learning\_rate}      & $[0.01, 0.1]$ \\
  & \texttt{l2\_leaf\_reg}       & $[1, 10]$ \\
  & \texttt{bagging\_temperature}& $[0.0, 1.0]$ \\
\hline
\end{tabular}
\end{table}


\section{Methodology}
Our modeling pipeline follows a systematic approach from baseline models to advanced ensembles.

\subsection{Validation Strategy}
To ensure reliable performance estimation and prevent overfitting:
\begin{itemize}
    \item \textbf{5-fold cross-validation} with shuffle (random seed = 42)
    \item Consistent fold splits across all models for fair comparison
    \item Out-of-fold (OOF) predictions stored for ensemble optimization
\end{itemize}

\subsection{Feature Engineering}
We expanded from 12 original features to \textbf{44 engineered features}:
\begin{itemize}
    \item \textbf{Interaction features}: study\_hours $\times$ class\_attendance, study\_hours $\times$ sleep\_hours, etc.
    \item \textbf{Ratio features}: attendance\_per\_hour, sleep\_per\_study
    \item \textbf{Polynomial features}: squared and cubed terms for key numerical variables
    \item \textbf{Binary flags}: is\_low\_sleep ($<$6h), is\_high\_attendance ($>$80\%), is\_high\_study ($>$6h)
    \item \textbf{Domain formula}: $6.0 \times \text{study\_hours} + 0.35 \times \text{attendance} + 1.5 \times \text{sleep\_hours}$
    \item \textbf{Target encoding}: CV-safe mean encoding for all categorical features
    \item \textbf{Frequency encoding}: Category frequency as additional features
\end{itemize}

\subsection{Models Evaluated}
We evaluated models of increasing complexity:

\textbf{Baseline Models:}
\begin{itemize}
    \item Ridge Regression ($\alpha = 10.0$) with standardized features
    \item Random Forest (100 trees, max\_depth=15, min\_samples\_leaf=10)
\end{itemize}

\textbf{Gradient Boosting Models:}
\begin{itemize}
    \item LightGBM (learning\_rate=0.05, num\_leaves=31, early stopping at 100 rounds)
    \item XGBoost (learning\_rate=0.05, max\_depth=6, tree\_method=hist)
    \item CatBoost (learning\_rate=0.05, depth=6, l2\_leaf\_reg=3)
\end{itemize}

\textbf{Ensemble Methods:}
\begin{itemize}
    \item Simple average of LightGBM, XGBoost, CatBoost predictions
    \item Weighted ensemble with Nelder-Mead optimized weights on OOF predictions
    \item Stacking ensemble with Ridge regression meta-learner
\end{itemize}

\textbf{Deep Learning Model (SE-ResNet):}
\begin{itemize}
    \item Neural network with entity embeddings for categorical features
    \item Residual blocks with Squeeze-and-Excitation attention mechanism
    \item Data augmentation using original dataset (not just competition data)
\end{itemize}

\subsection{SE-ResNet Architecture}
Our neural network architecture combines several modern techniques for tabular data:

\textbf{Input Processing:}
\begin{itemize}
    \item Numerical features: StandardScaler normalization, plus engineered features (log transforms, squared terms, sinusoidal encodings, frequency counts)
    \item Categorical features: Learnable embeddings with dimension $d=8$ per category
    \item Domain formula: $5.91 \times \text{study\_hours} + 0.35 \times \text{attendance} + 1.42 \times \text{sleep\_hours} + 4.78$
\end{itemize}

\textbf{Network Architecture:}
\begin{itemize}
    \item Projection layer: Concatenated inputs $\rightarrow$ 256 hidden dimensions
    \item 3 residual blocks, each containing:
    \begin{itemize}
        \item LayerNorm $\rightarrow$ Linear $\rightarrow$ ReLU $\rightarrow$ Dropout (0.11)
        \item SE block with reduction ratio $r=4$
        \item Skip connection
    \end{itemize}
    \item Prediction head: 256 $\rightarrow$ 64 $\rightarrow$ 16 $\rightarrow$ 1
\end{itemize}

\textbf{Training Configuration:}
\begin{itemize}
    \item Optimizer: AdamW (lr=$10^{-3}$, weight decay=$10^{-4}$)
    \item Scheduler: ReduceLROnPlateau (factor=0.5, patience=10)
    \item Early stopping: patience=20 epochs
    \item Batch size: 256 (train), 1024 (validation)
    \item Data augmentation: Original dataset concatenated with training fold
\end{itemize}

\subsection{Hyperparameter Tuning}
We tuned the gradient boosting models using Bayesian optimization with Optuna~\cite{optuna}.
For each model, we ran 50 trials and evaluated each trial using 5-fold cross-validation, minimizing the RMSE.
The hyperparameter ranges explored for LightGBM, XGBoost, and CatBoost are summarized in Table~\ref{tab:search_space}.
Unless stated otherwise, parameters were sampled uniformly within their ranges, while \texttt{learning\_rate}, \texttt{reg\_alpha}, and \texttt{reg\_lambda} were sampled log-uniformly to better cover multiple orders of magnitude.
The best-performing configurations selected by Optuna for each model are reported in Table~\ref{tab:tuned_params}.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.1}
\caption{Optuna search spaces used for Bayesian hyperparameter optimization.}
\label{tab:search_space}
\begin{tabular}{l l l}
\hline
\textbf{Model} & \textbf{Hyperparameter} & \textbf{Range} \\
\hline
\multirow{7}{*}{LightGBM}
  & \texttt{num\_leaves}         & $[20, 100]$ \\
  & \texttt{learning\_rate}      & $[0.01, 0.1]$ \\
  & \texttt{feature\_fraction}   & $[0.6, 1.0]$ \\
  & \texttt{bagging\_fraction}   & $[0.6, 1.0]$ \\
  & \texttt{min\_child\_samples} & $[5, 50]$ \\
  & \texttt{reg\_alpha}          & $[10^{-3}, 10]$ \\
  & \texttt{reg\_lambda}         & $[10^{-3}, 10]$ \\
\hline
\multirow{6}{*}{XGBoost}
  & \texttt{max\_depth}          & $[3, 10]$ \\
  & \texttt{learning\_rate}      & $[0.01, 0.1]$ \\
  & \texttt{subsample}           & $[0.6, 1.0]$ \\
  & \texttt{colsample\_bytree}   & $[0.6, 1.0]$ \\
  & \texttt{reg\_alpha}          & $[10^{-3}, 10]$ \\
  & \texttt{reg\_lambda}         & $[10^{-3}, 10]$ \\
\hline
\multirow{4}{*}{CatBoost}
  & \texttt{depth}               & $[4, 10]$ \\
  & \texttt{learning\_rate}      & $[0.01, 0.1]$ \\
  & \texttt{l2\_leaf\_reg}       & $[1, 10]$ \\
  & \texttt{bagging\_temperature}& $[0.0, 1.0]$ \\
\hline
\end{tabular}
\end{table}


\section{Conclusion}
This project addressed the Kaggle Playground Series S6E1 challenge of predicting student exam scores from tabular demographic and behavioral data.

\subsection{Summary of Contributions}
\begin{itemize}
    \item Developed a comprehensive feature engineering pipeline expanding 12 original features to 44 engineered features.
    \item Compared 8+ modeling approaches spanning linear models, tree ensembles, gradient boosting, and deep learning.
    \item Implemented Bayesian hyperparameter optimization with Optuna (50 trials per model).
    \item Achieved a final CV RMSE of \textbf{8.6047} using SE-ResNet neural network, outperforming the best gradient boosting ensemble by 1.74\%.
    \item Provided model interpretability analysis using SHAP values for gradient boosting models.
\end{itemize}

\subsection{Key Findings}
\begin{enumerate}
    \item \textbf{Deep learning can beat GBMs on tabular data}: The SE-ResNet architecture with entity embeddings and attention mechanisms achieved the best results, challenging the conventional wisdom that gradient boosting always wins on tabular data.
    \item \textbf{Architectural design matters}: The combination of embeddings, attention mechanisms, and residual connections enabled the neural network to outperform tree-based models.
    \item \textbf{Feature engineering matters}: The engineered \texttt{formula\_score} dominated feature importance across all models.
    \item \textbf{Attention helps}: Squeeze-and-Excitation blocks enable instance-specific feature weighting, improving predictions.
    \item \textbf{GBM ensembles have diminishing returns}: Weighted blending and stacking improved GBMs by only $\sim$0.005 RMSE.
\end{enumerate}

\subsection{Future Work}
Potential improvements include:
\begin{itemize}
    \item Exploring alternative neural architectures (TabNet, FT-Transformer, NODE)
    \item Hybrid ensembles combining GBM and neural network predictions
    \item Investigating neural network interpretability methods (e.g., attention visualization, integrated gradients)
\end{itemize}

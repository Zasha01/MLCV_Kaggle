\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% -------------------- Packages --------------------
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}
\usepackage{float}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Figures live under latex/figures (copied from outputs/figures)
\graphicspath{{figures/}}

% -------------------- Title --------------------
\title{Predicting Student Exam Scores Using Tabular Machine Learning\\
\thanks{University Kaggle Competition Report (Playground Series S6E1)}
}

% -------------------- Authors --------------------
\author{
\IEEEauthorblockN{Student 1}
\IEEEauthorblockA{\textit{Department} \\
\textit{University Name}\\
City, Country \\
email@domain.com}
\and
\IEEEauthorblockN{Tina Kovačević}
\IEEEauthorblockA{\textit{FEUP} \\
\textit{University of Porto}\\
Porto, Portugal \\
up202501724@up.pt}
\and
\IEEEauthorblockN{Zakariea Sharfeddine}
\IEEEauthorblockA{\textit{FEUP} \\
\textit{University of Porto}\\
Porto, Portugal \\
up202501730@up.pt}
}

\begin{document}
\maketitle

% -------------------- Abstract --------------------
\begin{abstract}
This project addresses the Kaggle Playground Series S6E1 competition: predicting student exam scores from demographic, academic, and behavioral features.
Using a dataset of 630,000 training samples with 12 original features, we developed a comprehensive machine learning pipeline including feature engineering, Bayesian hyperparameter optimization, ensemble methods, and deep learning.
We evaluated traditional models (Ridge, Random Forest), gradient boosting methods, e.g. LightGBM, XGBoost, CatBoost), and a novel SE-ResNet neural network architecture combining entity embeddings, residual connections, and Squeeze-and-Excitation attention.
Our best solution, the SE-ResNet with data augmentation, achieved a cross-validation RMSE of \textbf{8.6047}, outperforming the best gradient boosting ensemble (8.7567) by 1.74\%.
SHAP analysis revealed that study hours and an engineered domain formula were the strongest predictors.
We discuss model interpretability, the advantages of neural networks for tabular data, and trade-offs between different approaches.
\end{abstract}

% -------------------- Keywords --------------------
\begin{IEEEkeywords}
tabular deep learning, gradient boosting, squeeze-and-excitation, entity embeddings, SHAP, ensemble learning
\end{IEEEkeywords}

% -------------------- Sections --------------------
\input{sections/introduction}
\input{sections/background}
\input{sections/data}
\input{sections/methodology}
\input{sections/experiments_results}
\input{sections/discussion}
\input{sections/conclusion}

% -------------------- References --------------------
\bibliographystyle{ieeetr}
\bibliography{references}

% -------------------- Appendix (optional) --------------------
\input{sections/appendix}

\end{document}
